diff --git a/README.md b/README.md
index 87dd8a2..03c32ae 100644
--- a/README.md
+++ b/README.md
@@ -94,7 +94,7 @@ And all output audios will be saved in dir data/saved/predict

 ## About model

-[`WANDB`](https://drive.google.com/file/d/1LoU_kCzl20hM5p709teRPB0a4Jib8VK-/view?usp=sharing)
+[`WANDB`](https://wandb.ai/rodion-chernomordin/neural_vocoder?nw=nwuserrodionchernomordin)

 [`Final model`](https://drive.google.com/file/d/1LoU_kCzl20hM5p709teRPB0a4Jib8VK-/view?usp=sharing)

diff --git a/src/configs/datasets/custom_inf.yaml b/src/configs/datasets/custom_inf.yaml
index 615d7ce..b71ce23 100644
--- a/src/configs/datasets/custom_inf.yaml
+++ b/src/configs/datasets/custom_inf.yaml
@@ -2,6 +2,6 @@ test:
   _target_: src.datasets.Ð¡ustomAudioDataset
   audio_size: -1
   dir: data/test
-  with_texts: False
+  with_texts: True
   with_audio: True
   instance_transforms: ${transforms.instance_transforms.inference}
diff --git a/src/configs/inference.yaml b/src/configs/inference.yaml
index 08b6861..6597db5 100644
--- a/src/configs/inference.yaml
+++ b/src/configs/inference.yaml
@@ -1,5 +1,6 @@
 defaults:
   - model: hifigan_v1
+  - writer: wandb
   - metrics: hifigan
   - datasets: custom_inf # we do not want to run inference on training data
   - dataloader: inference
@@ -9,5 +10,6 @@ inferencer:
   device_tensors: ["audio_data_object", "mel_spect_data_object"] # which tensors should be on device (ex. GPU)
   device: auto # device name or "auto"
   save_path: "predict" # any name here, can be a dataset name
+  override: True
   seed: 1
-  from_pretrained: "saved/checkpoint-epoch135.pth" # path to the pretrained model
+  from_pretrained: "saved/model_best.pth" # path to the pretrained model
diff --git a/src/configs/inference_from_text.yaml b/src/configs/inference_from_text.yaml
index 275ad6e..00b74ab 100644
--- a/src/configs/inference_from_text.yaml
+++ b/src/configs/inference_from_text.yaml
@@ -1,4 +1,5 @@
 defaults:
+  - writer: wandb
   - model: hifigan_v1
   - metrics: hifigan
   - datasets: custom_inf_from_text # we do not want to run inference on training data
@@ -9,5 +10,7 @@ inferencer:
   device_tensors: ["audio_data_object", "mel_spect_data_object"] # which tensors should be on device (ex. GPU)
   device: auto # device name or "auto"
   save_path: "predict" # any name here, can be a dataset name
+  override: True
   seed: 1
-  from_pretrained: "saved/checkpoint-epoch135.pth" # path to the pretrained model
+  from_pretrained: "saved/checkpoint-epoch175.pth" # path to the pretrained model
+  # from_pretrained: "saved/model_best-2.pth" # path to the pretrained model
\ No newline at end of file
diff --git a/src/trainer/inferencer.py b/src/trainer/inferencer.py
index c178f2d..350aab8 100644
--- a/src/trainer/inferencer.py
+++ b/src/trainer/inferencer.py
@@ -6,6 +6,10 @@ from src.metrics.tracker import MetricTracker
 from src.trainer.base_trainer import BaseTrainer
 from src.transforms.mel_spect import MelSpectrogramConfig

+from src.logger.utils import plot_spectrogram
+from random import shuffle
+import pandas as pd
+

 class Inferencer(BaseTrainer):
     """
@@ -23,6 +27,7 @@ class Inferencer(BaseTrainer):
         device,
         dataloaders,
         save_path,
+        writer,
         metrics=None,
         batch_transforms=None,
         skip_model_load=False,
@@ -58,6 +63,8 @@ class Inferencer(BaseTrainer):

         self.device = device

+        self.writer = writer
+
         self.model = model
         self.batch_transforms = batch_transforms

@@ -179,5 +186,41 @@ class Inferencer(BaseTrainer):
                     part=part,
                     metrics=self.evaluation_metrics,
                 )
+                self._log_spectrogram(**batch)
+                self._log_preds(**batch)

         return self.evaluation_metrics.result()
+
+    def _log_spectrogram(self, mel_spect_data_object, mel_spec_pred, **batch):
+        spectrogram_for_plot = mel_spect_data_object[0].detach().cpu()
+        image = plot_spectrogram(spectrogram_for_plot)
+        self.writer.add_image("true_spectrogram_" + batch["id"][0], image)
+
+        spectrogram_for_plot = mel_spec_pred[0].detach().cpu()
+        image = plot_spectrogram(spectrogram_for_plot)
+        self.writer.add_image("pred_spectrogram_" + batch["id"][0], image)
+
+    def _log_preds(self, examples_to_log=50, **batch):
+        result = {}
+        examples_to_log = min(examples_to_log, batch["audio_pred"].shape[0])
+
+        tuples = list(
+            zip(batch["audio_pred"], batch["audio_data_object"], batch["text"], batch["id"])
+        )
+        shuffle(tuples)
+
+        for idx, (pred, target, text, id) in enumerate(tuples[:examples_to_log]):
+            result[idx] = {
+                "predict_audio_" + id: self.writer.wandb.Audio(
+                    pred.squeeze(0).detach().cpu().numpy(),
+                    sample_rate=MelSpectrogramConfig.sr,
+                ),
+                "target_audio_" + id: self.writer.wandb.Audio(
+                    target.squeeze(0).detach().cpu().numpy(),
+                    sample_rate=MelSpectrogramConfig.sr,
+                ),
+                "text_" + id: text,
+            }
+        self.writer.add_table(
+            "predictions_" + batch["id"][0], pd.DataFrame.from_dict(result, orient="index")
+        )
diff --git a/src/utils/init_utils.py b/src/utils/init_utils.py
index 6467faa..277ed64 100644
--- a/src/utils/init_utils.py
+++ b/src/utils/init_utils.py
@@ -114,12 +114,12 @@ def saving_init(save_dir, config):
     run_id = None

     if save_dir.exists():
-        if config.trainer.get("resume_from") is not None:
+        if config.inferencer.get("resume_from") is not None:
             run_id = resume_config(save_dir)
-        elif config.trainer.override:
+        elif config.inferencer.override:
             print(f"Overriding save directory '{save_dir}'...")
             shutil.rmtree(str(save_dir))
-        elif not config.trainer.override:
+        elif not config.inferencer.override:
             raise ValueError(
                 "Save directory exists. Change the name or set override=True"
             )
@@ -149,10 +149,10 @@ def setup_saving_and_logging(config):
     Returns:
         logger (Logger): logger that logs output.
     """
-    save_dir = ROOT_PATH / config.trainer.save_dir / config.writer.run_name
+    save_dir = ROOT_PATH / config.inferencer.save_path / config.writer.run_name
     saving_init(save_dir, config)

-    if config.trainer.get("resume_from") is not None:
+    if config.inferencer.get("resume_from") is not None:
         setup_logging(save_dir, append=True)
     else:
         setup_logging(save_dir, append=False)
diff --git a/synthesize.py b/synthesize.py
index fbd4afe..4dc90e2 100644
--- a/synthesize.py
+++ b/synthesize.py
@@ -6,11 +6,13 @@ from hydra.utils import instantiate

 from src.datasets.data_utils import get_dataloaders
 from src.trainer import Inferencer
-from src.utils.init_utils import set_random_seed
+from src.utils.init_utils import set_random_seed, setup_saving_and_logging
 from src.utils.io_utils import ROOT_PATH

 warnings.filterwarnings("ignore", category=UserWarning)

+from omegaconf import OmegaConf
+

 @hydra.main(version_base=None, config_path="src/configs", config_name="inference")
 def main(config):
@@ -47,6 +49,10 @@ def main(config):
     save_path = ROOT_PATH / "data" / "saved" / config.inferencer.save_path
     save_path.mkdir(exist_ok=True, parents=True)

+    project_config = OmegaConf.to_container(config)
+    logger = setup_saving_and_logging(config)
+    writer = instantiate(config.writer, logger, project_config)
+
     inferencer = Inferencer(
         model=model,
         config=config,
@@ -55,6 +61,7 @@ def main(config):
         batch_transforms=batch_transforms,
         save_path=save_path,
         metrics=metrics,
+        writer=writer,
         skip_model_load=False,
     )
